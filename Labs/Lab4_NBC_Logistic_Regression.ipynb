{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab4: Naive Bayes Classifier & Logistic Regression**\n",
    "\n",
    "**Course**: **CSCI-866-001: Data Mining & Knowledge Discovery** <br>\n",
    "**Lecturer**: **Sothea HAS, PhD**\n",
    "\n",
    "-----\n",
    "\n",
    "**Objective:** In this lab, you will learn how to build NBC and Binary Logistic Regression model to predict `heart failure` patients. Not only that, you will learn to detect informative features for maximizing the potential of the constructed models. You will also see that **quantitative features** are not always the most important ones in building a good predictive model. You have to treat all types of data carefully.\n",
    "\n",
    "- The `notebook` of this `Lab` can be downloaded here: [Lab4_NBC_Logistic_Regression.ipynb](https://hassothea.github.io/Data_Analysis_AUPP/Labs/Lab4_NBC_Logistic_Regression.ipynb){target=\"_blank\"}.\n",
    "\n",
    "- Or you can work directly with `Google Colab` here: [Lab4_NBC_Logistic_Regression.ipynb](https://colab.research.google.com/drive/1MF5QMBMu-imuy7ffkUgiSF1EYF28Lajr?usp=sharing){target=\"_blank\"}.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to Heart Failure Prediction**\n",
    "\n",
    "[Cardiovascular diseases (CVDs)](https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1) are the leading cause of death globally, taking an estimated 17.9 million lives each year ([WHO](https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1)). CVDs are a group of disorders of the heart and blood vessels and include coronary heart disease, cerebrovascular disease, rheumatic heart disease and other conditions. More than four out of five CVD deaths are due to heart attacks and strokes, and one third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n",
    "\n",
    "The following `Heart Failure` dataset is obtained by combining 5 different heart disease datasets, consisting of 11 features and a target column indicating heart disease status of the patients. We will build a classification model to predict the heart status of the patients.\n",
    "\n",
    "We will explore [Kaggle Heart Failure Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction). Load the dataset into the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hasso\\.cache\\kagglehub\\datasets\\fedesoriano\\heart-failure-prediction\\versions\\1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"fedesoriano/heart-failure-prediction\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "data = pd.read_csv(path + \"/heart.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Univariate Analysis: Preprocessing & Data Analysis**\n",
    "\n",
    "- Check and modify if there are any columns with inappropriate data type.\n",
    "- Compute descriptive statistics of each column. Do you observe anything strange? Handle what seems to be the problem properly.\n",
    "- Are there any `nan` or `NA` values in this dataset?\n",
    "- Are their any duplicated observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Bivariate Analysis: Exploratory Data Analysis & Important Feature Detection**\n",
    "\n",
    "- Visualize the connection between all quantitative columns with `HeartDisease`. Notice those that seem to be related to the target.\n",
    "\n",
    "- Visualize the connection between all qualitative columns to the target. Note if there is any interesting qualitative columns (related to the target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Naive Bayes Classifier**\n",
    "\n",
    "- In the following code,  `train_test_split` was imported from `sklearn.model_selection`. It can be used to split the dataset into 80%-training (for constructing the model) and 20%-testing data (for testing model performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build NBC model on the 80%-training data using\n",
    "    - Only quantitative columns (name it `nbc_quan`)\n",
    "    - Only qualitative columns (name it `nbc_qual`). Hint: you should encode the categorical columns using `LabelEncoder` from `sklearn.preprocessing`.\n",
    "    - All columns (name it `nbc_full`). Hint: make sure you encode categorical columns using one-hot encoding then the `GaussianNB` can be applied on the encoded features.\n",
    "    - Your selected columns from the previous analyzing step (name it `nbc_analysis`).\n",
    "- Construct confusion matrix for the four models and compute the following metrics on the 20%-testing data:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1-score\n",
    "    - AUC\n",
    "- Which model seem to be the most promising one? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Binary Logistic Regression**\n",
    "\n",
    "- We start off from **feature engineering:**\n",
    "    - Standardize the quantitative inputs\n",
    "    - Perform one-hot encoding for all the qualitative variables.\n",
    "- Construct 4 Binary Logistic Regression models on the 80%-Training using different options of inputs as in the previous case.\n",
    "- Measure their performance on the corresponding testing dataset.\n",
    "- Conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Beyond Original Features (optional)**\n",
    "\n",
    "If you think that's all the models can do, you are probably wrong because the models have only been constructed on the original features. We probably can push the models' performances a little bit more by:\n",
    "- For logistic regression, the parameters $\\beta_j$ of the model can be restrict/penalized further to prevent overfitting (the model becomes to flexible).\n",
    "- Or introducing new features by transforming the original features. Be cautious because these new features might weaken the interpretability of the models.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "- **Penalty parameter C:** Try varying parameter $C$, for example, $C=0.01$ as follow `LogisticRegression(C=0.01)`. Fit the model to the training data then test its performance on the testing data. \n",
    "- **Search for the best $C$:** Now, try to search for the best $C$ and report the performance on the test data of the model built with the optimal value of $C$.\n",
    "- **Quadratic features:** the following codes generate quadratic selected features i.e., $X_1, X_2, X_3\\to X_1^2, X_2^2, X_3^3, X_1X_2, X_1X_3, X_2X_3$. When more features are created, the model will naturally become too flexible, it's recommended to fine-tune penalty parameter $C$ in this case as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Further Reading**\n",
    "\n",
    "$^{\\text{📚}}$  `Pandas` python library: [https://pandas.pydata.org/docs/getting_started/index.html#getting-started](https://pandas.pydata.org/docs/getting_started/index.html#getting-started) <br>\n",
    "$^{\\text{📚}}$  `Pandas Cheatsheet`: [https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) <br>\n",
    "$^{\\text{📚}}$  `10 Minute to Pandas`: [https://pandas.pydata.org/docs/user_guide/10min.html](https://pandas.pydata.org/docs/user_guide/10min.html) <br>\n",
    "$^{\\text{📚}}$  `Some Pandas Lession`: [https://www.kaggle.com/learn/pandas](https://pandas.pydata.org/docs/user_guide/10min.html) <br>\n",
    "$^{\\text{📚}}$ [Chapter 4, *Introduction to Statistical Learning with R*, James et al. (2021).](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf){target=\"_blank\"}. <br>\n",
    "$^{\\text{📚}}$ [Chapter 2, *The Elements of Statistical Learning*, Hastie et al. (2008).](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf){target=\"_blank\"}. <br>\n",
    "$^{\\text{📚}}$ [Friedman (1989)](http://www.leg.ufpr.br/~eferreira/CE064/Regularized%20Discriminant%20Analysis.pdf){target=\"_blank\"}. <br>\n",
    "$^{\\text{📚}}$ [Heart Disease Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset){target=\"_blank\"}. <br>\n",
    "$^{\\text{📚}}$ [Different Type of Correlation Metrics Used by Data Scientists, Ashray](https://www.analyticsvidhya.com/blog/2021/09/different-type-of-correlation-metrics-used-by-data-scientist/){target=\"_blank\"}. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
